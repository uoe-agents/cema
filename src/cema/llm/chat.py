""" This module is responsible for managing the chat interactions with the various models. """
import logging
import json
from itertools import chain
from typing import List, Dict, Any, Tuple

import openai
from vllm import LLM, SamplingParams


logger = logging.getLogger(__name__)
SYSTEM_PROMPT = "You are the explanation module of an autonomous driving system."


class ChatHandlerConfig:
    """ Configuration class for the ChatHandler. """

    def __init__(self, config: Dict[str, Any]):
        """ Initialize the ChatHandlerConfig.

        Args:
            config: The configuration dictionary.
        """
        self._config = config

    @property
    def model(self) -> str:
        """ Return the model name. """
        return self._config["model"]

    @property
    def model_kwargs(self) -> Dict[str, Any]:
        """ Return the model parameters. """
        return self._config.get("model_kwargs", {})

    @property
    def sampling_params(self) -> SamplingParams:
        """ Return the sampling parameters. """
        return SamplingParams(**self._config.get("sampling_params", {}))

    @property
    def system_prompt(self) -> str:
        """ Return the system prompt. """
        return self._config.get("system_prompt", SYSTEM_PROMPT)


class ChatHandler:
    """ Base class for handling chat interactions with various model. """

    def __init__(self,
                 model: str,
                 sampling_params: SamplingParams,
                 system_prompt: str,
                 model_kwargs: Dict[str, Any] = None):
        """ Initialize the ChatHandler.

        Args:
            model: The full HuggingFace model name to use for the chat handler.
            sampling_params: The sampling parameters to use for the model.
            system_prompt: The system prompt to use for the chat handler.
            model_kwargs: Additional keyword arguments for the specific model.
        """
        # Create model-related fields
        if model_kwargs is None:
            model_kwargs = {}
        self._model = self.load_model(model=model, **model_kwargs)
        self._sampling_params = sampling_params

        # Create prompting related fields
        if system_prompt is None:
            system_prompt = SYSTEM_PROMPT
        self._system_prompt = system_prompt
        self._user_history = []
        self._assistant_history = []
        self._response_history = []

        logger.info("[SYSTEM] %s", system_prompt)

    def create_message(self, prompt: str, use_history: bool = True) -> List[Dict[str, str]]:
        """ Create message to be sent to model, potentially using chat history.

        Args:
            prompt: The prompt to respond to.
            use_history: Whether to use the chat history as context.
            **kwargs: Additional keyword arguments for the model.
        """
        # Setup message history if needed
        messages = [
            {"role": "system", "content": self.system_prompt},
        ] + self.history if use_history else []

        # Create user message
        user_message = {"role": "user", "content": prompt}
        messages.append(user_message)
        self._user_history.append(user_message)

        return messages

    def interact(self, prompt: str, **kwargs) -> Tuple[List[str], List[Any]]:
        """ Interact with the model and return the response.

        Args:
            prompt: The prompt to respond to.
            **kwargs: Additional keyword arguments for the model.

        Returns:
            The response from the model and any additional information generated by the model.
        """
        messages = self.create_message(prompt)

        logger.info("[USER] %s", prompt[-1])
        answers, responses = self.prompt_model(messages, **kwargs)
        logger.info("\n[ASSISTANT] %s", answers)

        self.save_response(answers, responses)

        return answers, responses

    def prompt_model(self, messages: List[Dict[str, str]], **kwargs) -> Tuple[List[str], List[Any]]:
        """ Send prompt to model and return response.

        Args:
            message: The conversation to use for prompting.
            **kwargs: Additional keyword arguments for the model.

        Returns:
            The output text(s) and any additional information generated by the model.
        """
        raise NotImplementedError

    def load_model(self, model: str, **kwargs) -> LLM:
        """ Load the model based on the given parameters.

        Args:
            model: The model name to load.
            **kwargs: Additional keyword arguments for the model.
        """
        raise NotImplementedError

    def reset_history(self):
        """ Reset all of the chat history. """
        self._user_history = []
        self._assistant_history = []
        self._response_history = []

    def save_response(self, answers: List[str], responses: List[Any]):
        """ Save the assistant message to the chat history. """
        self.response_history.append(responses)
        wrapped_message = [{"role": "assistant", "content": r} for r in answers]
        self._assistant_history.append(wrapped_message)

    def write_chat_history(self, path: str):
        """ Write the chat history to a file. """
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.history, f)

    def write_response_history(self, path: str):
        """ Write the chat history to a file. """
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.response_history, f)

    @property
    def model(self):
        """ Return the model/client used by the chat handler. """
        return self._model

    @property
    def sampling_params(self) -> SamplingParams:
        """ Return the sampling parameters used by the chat handler. """
        return self._sampling_params

    @property
    def history(self) -> Dict[str, List[Dict[str, str]]]:
        """ Return the combined user and assisstant chat history as alternating messages.
         Using the first answer among the assistant's responses. """
        assistant_history = [r[0] for r in self.response_history]
        return list(chain.from_iterable(zip(self.user_history, assistant_history)))

    @property
    def user_history(self) -> List[Dict[str, str]]:
        """ Return the message history of the user. """
        return self._user_history

    @property
    def assistant_history(self) -> List[str]:
        """ Return the message history of the assistant. """
        return self._assistant_history

    @property
    def response_history(self) -> List[Dict[str, str]]:
        """ Return the full response history of the assistant. """
        return self._response_history

    @property
    def system_prompt(self) -> str:
        """ Return the system prompt. """
        return self._system_prompt


class LLMChatHandler(ChatHandler):
    """ Handle chat interactions with any offline LLM model. """

    def load_model(self, model, **kwargs):
        return LLM(model, **kwargs)

    def prompt_model(self, messages: List[Dict[str, str]], **kwargs):
        response = self.model.chat(
            messages, self.sampling_params, use_tqdm=False, **kwargs)

        return [o.text for o in response[0].outputs], response


class OpenAIChatHandler(ChatHandler):
    """ Handle chat interactions with the OpenAI API. """

    def load_model(self, model: str, **kwargs):
        logging.getLogger("openai").setLevel(logging.WARNING)
        logging.getLogger("httpcore").setLevel(logging.WARNING)
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("urllib3").setLevel(logging.WARNING)

        self._client = openai.OpenAI(api_key=openai.api_key)
        return model

    def prompt_model(self, messages: List[Dict[str, str]], **kwargs) -> Tuple[str, Any]:
        # Send message to API and store response if not empty
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        return [c.message.content for c in response.choices], response

    @property
    def client(self):
        """ Return the OpenAI client. Same as self.model. """
        return self._client


class ChatHandlerFactory:
    """ Factory class for creating ChatHandlers. """
    HANDLERS = {
        "base": LLMChatHandler,
        "openai": OpenAIChatHandler
    }

    @classmethod
    def create_chat_handler(cls, model_class: str, config: ChatHandlerConfig) -> ChatHandler:
        """ Create a new ChatHandler based on the given parameters.

        Args:
            config: The configuration for the chat handler.
        """
        if model_class not in cls.HANDLERS:
            logger.warning("Model %s not found among handlers. Using LLMChatHandler.", model_class)
        return cls.HANDLERS.get(model_class, LLMChatHandler)(
            config.model,
            config.sampling_params,
            config.system_prompt,
            config.model_kwargs)
